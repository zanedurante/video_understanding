# This is not currently used, but it is how I will configure the codebase
# TODO: Load with OmegaConf from default.yaml
data:
  datasets: 
    - 500p_new0 # contains the re-annotated videos in training split
  num_frames: 16

model:
  type: captioner # which module to use
  backbone_name: VideoMAEv2Base # clip_ViT-B/32
  backbone_pretrained_ckpt: /data/output/vit_b_hybrid_pt_800e/checkpoint-51.pth
  text_decoder_name: opt_125m
  head: linear
  head_dropout: 0.10
  text_first: True
  num_learnable_prompt_tokens: 8
  use_start_token_for_caption: True
  prompt: "A video of a "

trainer:
  lr: 1e-4
  batch_size: 2
  max_epochs: 16
  num_frozen_epochs: 0
  backbone_lr_multiplier: 0.01
  text_decoder_lr_multiplier: 0.2
  head_weight_decay: 0.0
  backbone_weight_decay: 0.0
  prompt_lr_multiplier: 0.01
  prompt_weight_decay: 0.0
  precision: 32
  num_workers: cpus-4
  accumulate_grad_batches: 32

logger:
  log_every_n_steps: 10
